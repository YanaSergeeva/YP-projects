{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pymystem3 import Mystem\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "toxic_comments = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "toxic_comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0\n",
       "5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7  Your vandalism to the Matt Shirvington article...      0\n",
       "8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9  alignment on this subject and which are contra...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(toxic_comments.shape)\n",
    "toxic_comments.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, в каком соотношений присутсвуют значения в целевом признаке toxic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.898321\n",
       "1    0.101679\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments['toxic'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.834884437596301"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_ratio = toxic_comments['toxic'].value_counts()[0] / toxic_comments['toxic'].value_counts()[1]\n",
    "class_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы несбалансированы. Отношение ~1:9 \n",
    "\n",
    "Проведем балансировку классов двумя способами: изменение весов в модели обучения и ресемплирование с уменьшением класса 0, после чего сравним качество"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим признаки и целевой признак перед обучением:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.8 s, sys: 9.03 s, total: 52.8 s\n",
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "m = Mystem()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = text.lower()\n",
    "    lemm_text = \"\".join(m.lemmatize(text))\n",
    "    cleared_text = re.sub(r'[^a-zA-Z]', ' ', lemm_text) \n",
    "    return \" \".join(cleared_text.split())\n",
    "\n",
    "toxic_comments['lemm_text'] = toxic_comments['text'].apply(lemmatize_text)\n",
    "\n",
    "toxic_comments = toxic_comments.drop(['text'], axis=1)\n",
    "del m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем выборку по отношению 60/20/20. Уменьшим количество кроссвалидаций до 3 из-за размера выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_train: (95742, 126292)\n",
      "features_valid: (31914, 126292)\n",
      "features_test: (31915, 126292)\n"
     ]
    }
   ],
   "source": [
    "target = toxic_comments['toxic']\n",
    "features = toxic_comments.drop(['toxic'], axis=1)\n",
    "\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(features, \n",
    "                                                                              target, \n",
    "                                                                              test_size=0.4, \n",
    "                                                                              random_state=12082020)\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(features_valid, \n",
    "                                                                            target_valid, \n",
    "                                                                            test_size=0.5,\n",
    "                                                                            random_state=12082020)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "features_train = count_tf_idf.fit_transform(features_train['lemm_text'].values.astype('U'))\n",
    "features_valid = count_tf_idf.transform(features_valid['lemm_text'].values.astype('U'))\n",
    "features_test = count_tf_idf.transform(features_test['lemm_text'].values.astype('U'))\n",
    "print('features_train:', features_train.shape)\n",
    "print('features_valid:', features_valid.shape)\n",
    "print('features_test:', features_test.shape)\n",
    "cv_counts = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на CV 0.6732278141449742\n",
      "F1 на валидации 0.7268891446106636\n",
      "CPU times: user 12.5 s, sys: 12.6 s, total: 25.1 s\n",
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classificator = LogisticRegression()\n",
    "train_f1 = cross_val_score(classificator,\n",
    "                      features_train,\n",
    "                      target_train,\n",
    "                      cv=cv_counts,\n",
    "                      scoring='f1').mean()\n",
    "classificator.fit(features_train, target_train)\n",
    "valid_f1 = f1_score(target_valid, classificator.predict(features_valid))\n",
    "\n",
    "print('F1 на CV', train_f1)\n",
    "print('F1 на валидации', valid_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Изменение весов в модели обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на CV с балансированными классами 0.7553768002693207\n",
      "F1 на валидации с балансированными классами 0.7586593745617726\n",
      "CPU times: user 30.3 s, sys: 26.1 s, total: 56.5 s\n",
      "Wall time: 56.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_classes={0:1, 1:class_ratio}\n",
    "classificator = LogisticRegression(class_weight=dict_classes)\n",
    "train_f1_ballanced = cross_val_score(classificator,\n",
    "                                    features_train,\n",
    "                                    target_train,\n",
    "                                    cv=cv_counts,\n",
    "                                    scoring='f1').mean()\n",
    "classificator.fit(features_train, target_train)\n",
    "valid_f1_balanced = f1_score(target_valid, classificator.predict(features_valid))\n",
    "\n",
    "print('F1 на CV с балансированными классами', train_f1_ballanced)\n",
    "print('F1 на валидации с балансированными классами', valid_f1_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на CV с балансированными классами 0.7553768002693207\n",
      "CPU times: user 17.5 s, sys: 15.7 s, total: 33.1 s\n",
      "Wall time: 33.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classificator = LogisticRegression(class_weight='balanced')\n",
    "train_f1_balanced = cross_val_score(classificator, \n",
    "                                    features_train, \n",
    "                                    target_train, \n",
    "                                    cv=cv_counts, \n",
    "                                    scoring='f1').mean()\n",
    "print('F1 на CV с балансированными классами', train_f1_ballanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно на обучающей выборке F1-мера увеличилась. Встроенный метод повторяет значение F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ресемплирование с уменьшением класса 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем количество записей с классом 0 таким же, как и количество записей с классом 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_comments_train = toxic_comments.iloc[target_train.index]\n",
    "\n",
    "target_train_class_zero = toxic_comments_train[toxic_comments_train['toxic'] == 0]['toxic']\n",
    "target_train_class_one = toxic_comments_train[toxic_comments_train['toxic'] == 1]['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_class_zero_downsample = target_train_class_zero.sample(target_train_class_one.shape[0],\n",
    "                                                                    random_state=12345)\n",
    "target_train_downsample = pd.concat([target_train_class_zero_downsample, target_train_class_one])\n",
    "\n",
    "features_train_downsample = toxic_comments.iloc[target_train_downsample.index]\n",
    "features_train_downsample, target_train_downsample = shuffle(features_train_downsample,\n",
    "                                                             target_train_downsample,\n",
    "                                                             random_state=12345)\n",
    "features_train_downsample = count_tf_idf.transform(features_train_downsample['lemm_text']\n",
    "                                                   .values.astype('U'))\n",
    "del count_tf_idf\n",
    "del stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на CV с уменьшением классов 0.8821013380800937\n",
      "F1 на валидации с уменьшением классов 0.6953144575294412\n"
     ]
    }
   ],
   "source": [
    "classificator = LogisticRegression()\n",
    "train_f1_downsampled = cross_val_score(classificator,\n",
    "                      features_train_downsample,\n",
    "                      target_train_downsample,\n",
    "                      cv=cv_counts,\n",
    "                      scoring='f1').mean()\n",
    "classificator.fit(features_train_downsample,target_train_downsample)\n",
    "valid_f1_downsampled = f1_score(target_valid, classificator.predict(features_valid))\n",
    "\n",
    "print('F1 на CV с уменьшением классов', train_f1_downsampled)\n",
    "print('F1 на валидации с уменьшением классов', valid_f1_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значение F1-меры существенно увеличилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 на CV</th>\n",
       "      <th>F1 на валидации</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.673228</td>\n",
       "      <td>0.726889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LR balansed classes</td>\n",
       "      <td>0.746746</td>\n",
       "      <td>0.758659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LR downsampled classes</td>\n",
       "      <td>0.882101</td>\n",
       "      <td>0.695314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        F1 на CV  F1 на валидации\n",
       "LogisticRegression      0.673228         0.726889\n",
       "LR balansed classes     0.746746         0.758659\n",
       "LR downsampled classes  0.882101         0.695314"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = ['LogisticRegression',\n",
    "         'LR balansed classes',\n",
    "         'LR downsampled classes']\n",
    "data = {'F1 на CV':[train_f1,\n",
    "                    train_f1_balanced,\n",
    "                    train_f1_downsampled],\n",
    "        'F1 на валидации':[valid_f1,\n",
    "                           valid_f1_balanced,\n",
    "                           valid_f1_downsampled]}\n",
    "\n",
    "scores_data = pd.DataFrame(data=data, index=index)\n",
    "scores_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По полученым результатом оптимальным показателем для F1 обладает классификатор, где учтен вес классов. Удалим ненужные переменные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del toxic_comments_train\n",
    "del target_train_class_zero\n",
    "del target_train_class_one\n",
    "del target_train_class_zero_downsample\n",
    "del target_train_downsample\n",
    "del features_train_downsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На тестировании оптимальным показателем для F1 обладает классификатор, где учтен вес классов. В обучении мы будем использовать именно этот метод балансирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исследуем следующие модели для обучения:\n",
    "\n",
    "- LogisticRegression\n",
    "- DecisionTreeClassifier\n",
    "- CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LogisticRegression:\n",
      "\n",
      "{'C': 10, 'class_weight': {0: 1, 1: 8.834884437596301}, 'solver': 'lbfgs'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.713  for {'C': 0.1, 'class_weight': {0: 1, 1: 8.834884437596301}, 'solver': 'newton-cg'}\n",
      "0.713  for {'C': 0.1, 'class_weight': {0: 1, 1: 8.834884437596301}, 'solver': 'lbfgs'}\n",
      "0.713  for {'C': 0.1, 'class_weight': {0: 1, 1: 8.834884437596301}, 'solver': 'liblinear'}\n",
      "0.755  for {'C': 1, 'class_weight': {0: 1, 1: 8.834884437596301}, 'solver': 'newton-cg'}\n",
      "0.755  for {'C': 1, 'class_weight': {0: 1, 1: 8.834884437596301}, 'solver': 'lbfgs'}\n",
      "0.755  for {'C': 1, 'class_weight': {0: 1, 1: 8.834884437596301}, 'solver': 'liblinear'}\n",
      "0.763  for {'C': 10, 'class_weight': {0: 1, 1: 8.834884437596301}, 'solver': 'newton-cg'}\n",
      "0.763  for {'C': 10, 'class_weight': {0: 1, 1: 8.834884437596301}, 'solver': 'lbfgs'}\n",
      "0.763  for {'C': 10, 'class_weight': {0: 1, 1: 8.834884437596301}, 'solver': 'liblinear'}\n",
      "\n",
      "CPU times: user 5min 43s, sys: 5min 37s, total: 11min 20s\n",
      "Wall time: 11min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classificator = LogisticRegression()\n",
    "hyperparams = [{'solver':['newton-cg', 'lbfgs', 'liblinear'],\n",
    "                'C':[0.1, 1, 10],\n",
    "                'class_weight':[dict_classes]}]\n",
    "\n",
    "\n",
    "clf = GridSearchCV(classificator, hyperparams, scoring='f1',cv=cv_counts)\n",
    "clf.fit(features_train, target_train)\n",
    "print(\"Best parameters for LogisticRegression:\")\n",
    "print()\n",
    "LR_best_params = clf.best_params_\n",
    "print(LR_best_params)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f  for %r\" % (mean, params))\n",
    "print()\n",
    "\n",
    "cv_f1_LR = max(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на cv для LogisticRegression 0.7631191168916892\n",
      "F1 на валидации для LogisticRegression 0.7647146475935032\n",
      "CPU times: user 25 s, sys: 25.5 s, total: 50.5 s\n",
      "Wall time: 50.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classificator = LogisticRegression()\n",
    "classificator.set_params(**LR_best_params)\n",
    "classificator.fit(features_train, target_train)\n",
    "target_predict = classificator.predict(features_valid)\n",
    "valid_f1_LR = f1_score(target_valid, target_predict)\n",
    "\n",
    "print('F1 на cv для LogisticRegression', cv_f1_LR)\n",
    "print('F1 на валидации для LogisticRegression', valid_f1_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parametrs for DecisionTreeClassifier:\n",
      "\n",
      "{'class_weight': {0: 1, 1: 8.834884437596301}, 'max_depth': 40, 'random_state': 12345}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.605 for {'class_weight': {0: 1, 1: 8.834884437596301}, 'max_depth': 30, 'random_state': 12345}\n",
      "0.609 for {'class_weight': {0: 1, 1: 8.834884437596301}, 'max_depth': 32, 'random_state': 12345}\n",
      "0.615 for {'class_weight': {0: 1, 1: 8.834884437596301}, 'max_depth': 34, 'random_state': 12345}\n",
      "0.615 for {'class_weight': {0: 1, 1: 8.834884437596301}, 'max_depth': 36, 'random_state': 12345}\n",
      "0.620 for {'class_weight': {0: 1, 1: 8.834884437596301}, 'max_depth': 38, 'random_state': 12345}\n",
      "0.627 for {'class_weight': {0: 1, 1: 8.834884437596301}, 'max_depth': 40, 'random_state': 12345}\n",
      "0.621 for {'class_weight': {0: 1, 1: 8.834884437596301}, 'max_depth': 42, 'random_state': 12345}\n",
      "0.606 for {'class_weight': {0: 1, 1: 8.834884437596301}, 'max_depth': 44, 'random_state': 12345}\n",
      "0.605 for {'class_weight': {0: 1, 1: 8.834884437596301}, 'max_depth': 46, 'random_state': 12345}\n",
      "0.608 for {'class_weight': {0: 1, 1: 8.834884437596301}, 'max_depth': 48, 'random_state': 12345}\n",
      "0.615 for {'class_weight': {0: 1, 1: 8.834884437596301}, 'max_depth': 50, 'random_state': 12345}\n",
      "\n",
      "CPU times: user 15min 40s, sys: 0 ns, total: 15min 40s\n",
      "Wall time: 15min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classificator = DecisionTreeClassifier()\n",
    "hyperparams = [{'max_depth':[x for x in range(30,51,2)],\n",
    "                'random_state':[12345],\n",
    "                'class_weight':[dict_classes]}]\n",
    "\n",
    "\n",
    "clf = GridSearchCV(classificator, hyperparams, scoring='f1',cv=cv_counts)\n",
    "clf.fit(features_train, target_train)\n",
    "print(\"Best parametrs for DecisionTreeClassifier:\")\n",
    "print()\n",
    "DTC_best_params = clf.best_params_\n",
    "print(DTC_best_params)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f for %r\"% (mean, params))\n",
    "print()\n",
    "\n",
    "cv_f1_DTC = max(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на cv для DecisionTreeClassifier 0.6266521659986855\n",
      "F1 на валидации для DecisionTreeClassifier 0.604064678831034\n",
      "CPU times: user 35.3 s, sys: 0 ns, total: 35.3 s\n",
      "Wall time: 35.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classificator = DecisionTreeClassifier()\n",
    "classificator.set_params(**DTC_best_params)\n",
    "classificator.fit(features_train, target_train)\n",
    "target_predict = classificator.predict(features_valid)\n",
    "valid_f1_DTC = f1_score(target_valid, target_predict)\n",
    "print('F1 на cv для DecisionTreeClassifier', cv_f1_DTC)\n",
    "print('F1 на валидации для DecisionTreeClassifier', valid_f1_DTC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на cv для CatBoostClassifier 0.7186686644372537\n",
      "F1 на валидации для CatBoostClassifier 0.7413539367181752\n",
      "CPU times: user 49min 5s, sys: 8min 8s, total: 57min 14s\n",
      "Wall time: 57min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classificator = CatBoostClassifier(verbose=False, iterations=200)\n",
    "classificator.fit(features_train, target_train)\n",
    "target_predict = classificator.predict(features_valid)\n",
    "cv_f1_CBC = cross_val_score(classificator,\n",
    "                                         features_train, \n",
    "                                         target_train, \n",
    "                                         cv=cv_counts, \n",
    "                                         scoring='f1').mean()\n",
    "valid_f1_CBC = f1_score(target_valid, target_predict)\n",
    "print('F1 на cv для CatBoostClassifier', cv_f1_CBC)\n",
    "print('F1 на валидации для CatBoostClassifier', valid_f1_CBC) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем таблицу валидации исследованных моделей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 на CV</th>\n",
       "      <th>F1 на валидации</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.763119</td>\n",
       "      <td>0.764715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.625155</td>\n",
       "      <td>0.605458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>0.718669</td>\n",
       "      <td>0.741354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        F1 на CV  F1 на валидации\n",
       "LogisticRegression      0.763119         0.764715\n",
       "DecisionTreeClassifier  0.625155         0.605458\n",
       "CatBoostClassifier      0.718669         0.741354"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = ['LogisticRegression',\n",
    "         'DecisionTreeClassifier',\n",
    "         'CatBoostClassifier']\n",
    "data = {'F1 на CV':[cv_f1_LR,\n",
    "                    cv_f1_DTC,\n",
    "                    cv_f1_CBC],\n",
    "        'F1 на валидации':[valid_f1_LR,\n",
    "                           valid_f1_DTC,\n",
    "                           valid_f1_CBC]}\n",
    "\n",
    "scores_data = pd.DataFrame(data=data, index=index)\n",
    "scores_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наивысшим показателем метрики F1 обладает модель LogisticRegression. Проведем тестирование данной модели: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики LogisticRegression:\n",
      "F1: 0.7650257163850109\n",
      "Accuracy: 0.94989816700611\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classificator = LogisticRegression()\n",
    "classificator.set_params(**LR_best_params)\n",
    "classificator.fit(features_train, target_train)\n",
    "predict_test = classificator.predict(features_test)\n",
    "print('Метрики LogisticRegression:')\n",
    "print('F1:', f1_score(target_test, predict_test))\n",
    "print('Accuracy:', accuracy_score(target_test, predict_test))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе работы над проектом было сделано:\n",
    "\n",
    "- В ходе преподготовки были получены признаки для обучения и разделена выборка на обучающую, валидационную и тестовую.\n",
    "- На тестировании оптимальным показателем для F1 обладает классификатор, где учтен вес классов. Для обучения использовали именно этот метод балансирования.\n",
    "- Обучили модели:\n",
    "    - LogisticRegression\n",
    "    - DecisionTreeClassifier\n",
    "    - CatBoostClassifier \n",
    "и  для кажой определили лучшие параметры качества.\n",
    "\n",
    "Исходные данные обладают большим количеством признаков. Созданных столбцов больше, чем записей данных. Так как TF-IDF превращают текст в численные значения, лучшей моделью стала LogisticRegression.\n",
    "\n",
    "На тестовой выбоке метрика F1 у LogisticRegression получилась равной 0.765. Также у данной модели хорошее значение метрики Accuracy - 0.95, это говорит нам, что токсичные комментарии находятся лучше."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1764,
    "start_time": "2021-10-18T03:27:36.170Z"
   },
   {
    "duration": 844,
    "start_time": "2021-10-18T03:28:03.162Z"
   },
   {
    "duration": 16,
    "start_time": "2021-10-18T03:29:01.197Z"
   },
   {
    "duration": 15,
    "start_time": "2021-10-18T03:38:58.709Z"
   },
   {
    "duration": 155977,
    "start_time": "2021-10-18T03:49:03.609Z"
   },
   {
    "duration": 125,
    "start_time": "2021-10-18T03:51:39.590Z"
   },
   {
    "duration": 16,
    "start_time": "2021-10-18T03:52:50.146Z"
   },
   {
    "duration": 17693,
    "start_time": "2021-10-18T03:53:21.039Z"
   },
   {
    "duration": 23601,
    "start_time": "2021-10-18T03:55:13.636Z"
   },
   {
    "duration": 41509,
    "start_time": "2021-10-18T03:55:37.241Z"
   },
   {
    "duration": 33399,
    "start_time": "2021-10-18T03:56:18.753Z"
   },
   {
    "duration": 55,
    "start_time": "2021-10-18T03:56:52.156Z"
   },
   {
    "duration": 8,
    "start_time": "2021-10-18T03:57:00.414Z"
   },
   {
    "duration": 6,
    "start_time": "2021-10-18T03:57:08.835Z"
   },
   {
    "duration": 1986,
    "start_time": "2021-10-18T03:57:56.614Z"
   },
   {
    "duration": 1246,
    "start_time": "2021-10-18T03:58:16.355Z"
   },
   {
    "duration": 398,
    "start_time": "2021-10-18T03:58:34.194Z"
   },
   {
    "duration": 6,
    "start_time": "2021-10-18T03:58:51.739Z"
   },
   {
    "duration": 810,
    "start_time": "2021-10-18T03:58:52.810Z"
   },
   {
    "duration": 11,
    "start_time": "2021-10-18T03:58:53.623Z"
   },
   {
    "duration": 10,
    "start_time": "2021-10-18T03:58:53.993Z"
   },
   {
    "duration": 13,
    "start_time": "2021-10-18T03:58:54.467Z"
   },
   {
    "duration": 155821,
    "start_time": "2021-10-18T03:58:55.491Z"
   },
   {
    "duration": 17513,
    "start_time": "2021-10-18T04:01:31.314Z"
   },
   {
    "duration": 23629,
    "start_time": "2021-10-18T04:01:48.831Z"
   },
   {
    "duration": 41793,
    "start_time": "2021-10-18T04:02:12.463Z"
   },
   {
    "duration": 33504,
    "start_time": "2021-10-18T04:02:54.258Z"
   },
   {
    "duration": 66,
    "start_time": "2021-10-18T04:03:27.764Z"
   },
   {
    "duration": 2016,
    "start_time": "2021-10-18T04:03:27.832Z"
   },
   {
    "duration": 9155,
    "start_time": "2021-10-18T04:03:29.852Z"
   },
   {
    "duration": 109,
    "start_time": "2021-10-18T04:03:43.113Z"
   },
   {
    "duration": 28270,
    "start_time": "2021-10-18T04:04:45.616Z"
   },
   {
    "duration": 15,
    "start_time": "2021-10-18T04:05:16.430Z"
   },
   {
    "duration": 6,
    "start_time": "2021-10-18T04:06:20.620Z"
   },
   {
    "duration": 1281,
    "start_time": "2021-10-20T03:12:38.266Z"
   },
   {
    "duration": 638,
    "start_time": "2021-10-20T03:12:39.549Z"
   },
   {
    "duration": 9,
    "start_time": "2021-10-20T03:12:40.189Z"
   },
   {
    "duration": 22,
    "start_time": "2021-10-20T03:12:40.200Z"
   },
   {
    "duration": 23,
    "start_time": "2021-10-20T03:12:40.224Z"
   },
   {
    "duration": 100253,
    "start_time": "2021-10-20T03:12:40.249Z"
   },
   {
    "duration": 11478,
    "start_time": "2021-10-20T03:14:20.504Z"
   },
   {
    "duration": 23492,
    "start_time": "2021-10-20T03:14:31.984Z"
   },
   {
    "duration": 41394,
    "start_time": "2021-10-20T03:14:55.478Z"
   },
   {
    "duration": 32804,
    "start_time": "2021-10-20T03:15:36.874Z"
   },
   {
    "duration": 26,
    "start_time": "2021-10-20T03:16:09.679Z"
   },
   {
    "duration": 1299,
    "start_time": "2021-10-20T03:16:09.707Z"
   },
   {
    "duration": 9144,
    "start_time": "2021-10-20T03:16:11.009Z"
   },
   {
    "duration": 27176,
    "start_time": "2021-10-20T03:16:20.155Z"
   },
   {
    "duration": 13,
    "start_time": "2021-10-20T03:16:47.333Z"
   },
   {
    "duration": 10,
    "start_time": "2021-10-20T03:16:47.347Z"
   },
   {
    "duration": 4,
    "start_time": "2021-10-20T05:09:28.288Z"
   },
   {
    "duration": 617,
    "start_time": "2021-10-20T05:09:30.990Z"
   },
   {
    "duration": 7,
    "start_time": "2021-10-20T05:09:31.610Z"
   },
   {
    "duration": 6,
    "start_time": "2021-10-20T05:09:31.820Z"
   },
   {
    "duration": 8,
    "start_time": "2021-10-20T05:09:32.016Z"
   },
   {
    "duration": 103755,
    "start_time": "2021-10-20T05:09:32.511Z"
   },
   {
    "duration": 11910,
    "start_time": "2021-10-20T05:11:16.269Z"
   },
   {
    "duration": 23206,
    "start_time": "2021-10-20T05:11:28.181Z"
   },
   {
    "duration": 41494,
    "start_time": "2021-10-20T05:11:51.389Z"
   },
   {
    "duration": 33099,
    "start_time": "2021-10-20T05:12:32.884Z"
   },
   {
    "duration": 25,
    "start_time": "2021-10-20T05:13:05.985Z"
   },
   {
    "duration": 1320,
    "start_time": "2021-10-20T05:13:06.012Z"
   },
   {
    "duration": 9313,
    "start_time": "2021-10-20T05:13:07.334Z"
   },
   {
    "duration": 89,
    "start_time": "2021-10-20T05:13:16.649Z"
   },
   {
    "duration": 560,
    "start_time": "2021-10-20T05:13:16.179Z"
   },
   {
    "duration": 559,
    "start_time": "2021-10-20T05:13:16.181Z"
   },
   {
    "duration": 558,
    "start_time": "2021-10-20T05:13:16.183Z"
   },
   {
    "duration": 556,
    "start_time": "2021-10-20T05:13:16.186Z"
   },
   {
    "duration": 552,
    "start_time": "2021-10-20T05:13:16.191Z"
   },
   {
    "duration": 551,
    "start_time": "2021-10-20T05:13:16.193Z"
   },
   {
    "duration": 551,
    "start_time": "2021-10-20T05:13:16.194Z"
   },
   {
    "duration": 550,
    "start_time": "2021-10-20T05:13:16.196Z"
   },
   {
    "duration": 9,
    "start_time": "2021-10-20T05:14:23.783Z"
   },
   {
    "duration": 5,
    "start_time": "2021-10-20T05:14:28.461Z"
   },
   {
    "duration": 614310,
    "start_time": "2021-10-20T05:14:30.150Z"
   },
   {
    "duration": 45511,
    "start_time": "2021-10-20T05:24:44.545Z"
   },
   {
    "duration": 635379,
    "start_time": "2021-10-20T05:25:30.059Z"
   },
   {
    "duration": 23861,
    "start_time": "2021-10-20T05:36:05.441Z"
   },
   {
    "duration": 2711262,
    "start_time": "2021-10-20T05:36:29.304Z"
   },
   {
    "duration": 8,
    "start_time": "2021-10-20T06:21:40.568Z"
   },
   {
    "duration": 44278,
    "start_time": "2021-10-20T06:21:40.578Z"
   },
   {
    "duration": 1729,
    "start_time": "2021-10-20T13:55:57.324Z"
   },
   {
    "duration": 772,
    "start_time": "2021-10-20T13:55:59.690Z"
   },
   {
    "duration": 14,
    "start_time": "2021-10-20T13:56:00.464Z"
   },
   {
    "duration": 17,
    "start_time": "2021-10-20T13:56:00.482Z"
   },
   {
    "duration": 15,
    "start_time": "2021-10-20T13:56:00.502Z"
   },
   {
    "duration": 155612,
    "start_time": "2021-10-20T13:56:00.761Z"
   },
   {
    "duration": 16940,
    "start_time": "2021-10-20T13:58:36.376Z"
   },
   {
    "duration": 23477,
    "start_time": "2021-10-20T13:58:53.319Z"
   },
   {
    "duration": 41499,
    "start_time": "2021-10-20T13:59:16.799Z"
   },
   {
    "duration": 33302,
    "start_time": "2021-10-20T13:59:58.301Z"
   },
   {
    "duration": 29,
    "start_time": "2021-10-20T14:00:31.605Z"
   },
   {
    "duration": 1914,
    "start_time": "2021-10-20T14:00:31.636Z"
   },
   {
    "duration": 8910,
    "start_time": "2021-10-20T14:00:33.553Z"
   },
   {
    "duration": 467,
    "start_time": "2021-10-20T14:00:42.466Z"
   },
   {
    "duration": 33,
    "start_time": "2021-10-20T14:00:42.903Z"
   },
   {
    "duration": 34,
    "start_time": "2021-10-20T14:00:42.904Z"
   },
   {
    "duration": 35,
    "start_time": "2021-10-20T14:00:42.905Z"
   },
   {
    "duration": 26,
    "start_time": "2021-10-20T14:00:42.915Z"
   },
   {
    "duration": 26,
    "start_time": "2021-10-20T14:00:42.916Z"
   },
   {
    "duration": 28,
    "start_time": "2021-10-20T14:00:42.916Z"
   },
   {
    "duration": 28,
    "start_time": "2021-10-20T14:00:42.917Z"
   },
   {
    "duration": 30,
    "start_time": "2021-10-20T14:00:42.917Z"
   },
   {
    "duration": 6,
    "start_time": "2021-10-20T14:01:46.802Z"
   },
   {
    "duration": 680977,
    "start_time": "2021-10-20T14:01:49.193Z"
   },
   {
    "duration": 50602,
    "start_time": "2021-10-20T14:13:10.262Z"
   },
   {
    "duration": 953628,
    "start_time": "2021-10-20T14:14:00.867Z"
   },
   {
    "duration": 35633,
    "start_time": "2021-10-20T14:29:54.499Z"
   },
   {
    "duration": 3443371,
    "start_time": "2021-10-20T14:30:30.135Z"
   },
   {
    "duration": 15,
    "start_time": "2021-10-20T15:27:53.508Z"
   },
   {
    "duration": 51888,
    "start_time": "2021-10-20T15:27:53.525Z"
   },
   {
    "duration": 943980,
    "start_time": "2021-10-20T15:28:45.416Z"
   },
   {
    "duration": 35383,
    "start_time": "2021-10-20T15:44:29.399Z"
   },
   {
    "duration": 1737,
    "start_time": "2021-10-21T05:50:02.444Z"
   },
   {
    "duration": 794,
    "start_time": "2021-10-21T05:50:05.331Z"
   },
   {
    "duration": 29,
    "start_time": "2021-10-21T05:50:06.128Z"
   },
   {
    "duration": 16,
    "start_time": "2021-10-21T05:50:06.161Z"
   },
   {
    "duration": 13,
    "start_time": "2021-10-21T05:50:06.303Z"
   },
   {
    "duration": 152973,
    "start_time": "2021-10-21T05:50:10.098Z"
   },
   {
    "duration": 16828,
    "start_time": "2021-10-21T05:52:43.073Z"
   },
   {
    "duration": 24061,
    "start_time": "2021-10-21T05:52:59.903Z"
   },
   {
    "duration": 41970,
    "start_time": "2021-10-21T05:53:23.966Z"
   },
   {
    "duration": 33168,
    "start_time": "2021-10-21T05:54:05.938Z"
   },
   {
    "duration": 56,
    "start_time": "2021-10-21T05:54:39.109Z"
   },
   {
    "duration": 1938,
    "start_time": "2021-10-21T05:54:39.167Z"
   },
   {
    "duration": 8898,
    "start_time": "2021-10-21T05:54:41.108Z"
   },
   {
    "duration": -96,
    "start_time": "2021-10-21T05:54:50.104Z"
   },
   {
    "duration": -103,
    "start_time": "2021-10-21T05:54:50.113Z"
   },
   {
    "duration": 31731,
    "start_time": "2021-10-21T05:57:59.190Z"
   },
   {
    "duration": 56496,
    "start_time": "2021-10-21T05:58:30.924Z"
   },
   {
    "duration": 33182,
    "start_time": "2021-10-21T05:59:27.424Z"
   },
   {
    "duration": 13168,
    "start_time": "2021-10-21T06:00:00.608Z"
   },
   {
    "duration": 12,
    "start_time": "2021-10-21T06:00:13.778Z"
   },
   {
    "duration": 12,
    "start_time": "2021-10-21T06:00:13.792Z"
   },
   {
    "duration": 1359,
    "start_time": "2021-10-22T03:46:23.000Z"
   },
   {
    "duration": 650,
    "start_time": "2021-10-22T03:46:24.361Z"
   },
   {
    "duration": 10,
    "start_time": "2021-10-22T03:46:25.014Z"
   },
   {
    "duration": 20,
    "start_time": "2021-10-22T03:46:25.025Z"
   },
   {
    "duration": 8,
    "start_time": "2021-10-22T03:46:25.047Z"
   },
   {
    "duration": 112244,
    "start_time": "2021-10-22T03:46:26.880Z"
   },
   {
    "duration": 15175,
    "start_time": "2021-10-22T03:48:19.126Z"
   },
   {
    "duration": 25102,
    "start_time": "2021-10-22T03:48:34.303Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
